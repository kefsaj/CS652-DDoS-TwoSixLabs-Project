{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twosixlabs.ipynb",
      "provenance": [],
      "mount_file_id": "1FXWuFZz1AjOvgeeI3bgsrlWW0clIKB0Y",
      "authorship_tag": "ABX9TyOJu8uPbH4Z5Bbv81EDNA5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kefsaj/CS652-DDoS-TwoSixLabs-Project/blob/main/Twosixlabs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGoxGohenSgA"
      },
      "source": [
        "------\n",
        "# TwoSixLabs DDoS CS652 Project\n",
        "\n",
        "------\n",
        "Modified by Kefin Sajan\n",
        "\n",
        "The aim for this project was to venture into solving DDoS attack that affect many systems.  \n",
        "\n",
        " Files used in this particular situation is Locatated: /content/drive/MyDrive/CS652101-Network-ArchProtocols/Twosixlabs \n",
        "\n",
        "Project requirements: https://pantelis.github.io/cs652/docs/cloud/projects/ddos-detection/\n",
        "\n",
        "------\n",
        "\n",
        "___For this project, data from various sourses were used___ \n",
        "\n",
        "This is based on \"Detecting DDoS Attacks with Machine Learning\" created by Chae Clark from twosixlabs.\n",
        "Based on: https://www.twosixlabs.com/detecting-ddos-attacks-with-machine-learning/ \n",
        "\n",
        "\n",
        "DataSet used to run this script was taken from \"Two Six Labs Internet Disruption Dataset\" By Chae Clark. Due to space limitations, specifically \"Indian Ocean Earthquake\" taking 215 MB and \"Italy Blackout\" taking 188.8 MB were the only datasets used in this particular situation. Some issues were presented when using other datasets, and script could not be run. \n",
        "- Two Six Labs Internet Disruption Dataset (BGP)\n",
        "  - https://osf.io/ywz4t/\n",
        "\n",
        "\n",
        "\n",
        "Maxmind's GeoLite database provides approximate location related to each IP address related to each CIDR block \n",
        " - MaxMindâ€™s GeoLite2 free database  \n",
        "  - https://dev.maxmind.com/geoip/geoip2/geolite2/ \n",
        "  \n",
        "------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxVPxKBBiOjD"
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "import numpy   as np\n",
        "import pandas  as pd\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOI88l9Y4zdc"
      },
      "source": [
        "from sklearn.decomposition   import PCA\n",
        "from sklearn.utils           import class_weight\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.ensemble        import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics         import log_loss, accuracy_score, roc_auc_score, matthews_corrcoef"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPxVJfoRcOyH"
      },
      "source": [
        "sklearn is used in this situation to use random forest classification algorithm as well as provide a eas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yel_eoAJlvor"
      },
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "random_state = 42"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wizCEkRl5lu8"
      },
      "source": [
        "A random number is generated and is used for random forest classification algorithm used in this scenerio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfBUlRegl47R"
      },
      "source": [
        "# Global Parameters\n",
        "DATA_FOLDER = \"/content/drive/MyDrive/CS652101-Network-ArchProtocols/Twosixlabs/data_100/\"\n",
        "LABEL_FILENAME = \"internet_disruptions.tsv\"\n",
        "CAUSES = [\"Natural Disaster\",\n",
        "          \"DDoS\",\n",
        "          \"Power Outage\",\n",
        "          \"BGP Update\",\n",
        "          \"Misconfiguration\",\n",
        "          \"Cable Cut\",\n",
        "          \"none\"]\n",
        "\n",
        "# User-Specifiable Parameters          \n",
        "CAUSE_INDEX  = 1      # which cause to evaluate\n",
        "DATA_LIMIT   = 1      # how many countries/cities/organizations to use (max 100)\n",
        "TRIALS       = 50     # trials\n",
        "STEP         = 3      # ensures no poisoning of results (DO NOT CHANGE)\n",
        "PCA_DIM      = 40     # projection dimension (should be <= DATA_LIMIT)\n",
        "LEARN_METHOD = 1      # If user would like to use Random Forest Classifier method LEARN_METHOD should be 1, otherwise 2  will be used if Logistic Regression is favored"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuqaPp9zBnja"
      },
      "source": [
        "Specifying user parameters such whether to Random Forest method or if Logistic Regression. As well as how many samples are created per each attack. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCvmx6jJl7lZ"
      },
      "source": [
        "def process_cause(x):\n",
        "    if CAUSES[CAUSE_INDEX].lower() in x.lower():\n",
        "        return \"positive_class\"\n",
        "    return \"negative_class\""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HumtPrZmEb6"
      },
      "source": [
        "def preprocess_data(data_folder=DATA_FOLDER,\n",
        "                    label_filename=LABEL_FILENAME,\n",
        "                    data_limit=DATA_LIMIT):\n",
        "    nides = pd.read_csv(\"/content/drive/MyDrive/CS652101-Network-ArchProtocols/Twosixlabs/internet_disruptions.tsv\", sep=\"\\t\")[[\"common name\", \"cause\"]].dropna()\n",
        "    for key in nides[\"common name\"].values:\n",
        "        nides = nides.append({\"common name\":f\"before_{key}\",\"cause\":\"none\"}, ignore_index=True)\n",
        "    nides[\"cause\"] = nides.cause.apply(process_cause)\n",
        "    keep_keys = [key for key,value in Counter(nides.cause.values).items()]\n",
        "    keep_keys = list(nides[nides.cause.isin(keep_keys)][\"common name\"].values.ravel())\n",
        "    nides = nides[nides[\"common name\"].isin(keep_keys)]\n",
        "\n",
        "    lbl = []\n",
        "    for key in keep_keys:\n",
        "        cause = nides[nides[\"common name\"] == key].cause.values[0]\n",
        "        lbl.append(cause)\n",
        "    y = np.zeros((len(lbl)),dtype=int)\n",
        "    for i, cause in enumerate(set(lbl)):\n",
        "        y[np.where(np.array(lbl)==cause)[0]] = i\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for i,key in enumerate(sorted(keep_keys)):\n",
        "        try:\n",
        "            tmp = pickle.load(open(f\"{data_folder}{key}_100.attr\", \"rb\"))\n",
        "            [X.append(tmp[j].values.T[:,:data_limit]) for j in range(3)]\n",
        "            [Y.append(y[i]) for j in range(3)]\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    data_full = np.array(X)\n",
        "    print(data_full.shape)\n",
        "\n",
        "    data_labels = np.vstack(Y).ravel()\n",
        "\n",
        "    data_pos = data_full[data_labels == 1]\n",
        "    data_neg = data_full[data_labels == 0]\n",
        "    np.random.shuffle(data_pos)\n",
        "    np.random.shuffle(data_neg)\n",
        "    min_len = np.min([len(data_pos), len(data_neg)])\n",
        "    X = np.vstack([data_pos[:min_len], data_neg[:min_len]])\n",
        "    y = np.vstack([np.ones((min_len,1)),np.zeros((min_len,1))])\n",
        "    return X,y\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMpmOij4oD1k"
      },
      "source": [
        "Down sampling is performed to control class inbalance. This is because all data is not part of the DDoS attack. As mentioned in TwoSixlabs article, several days of data were also included. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7q4x2oomA4Z"
      },
      "source": [
        "def display_results(acc_test,auc_test,mcc_test):\n",
        "    print(\"mean +/- std [min, max]\")\n",
        "    print(f\"{np.round(np.mean(acc_test),4)} +/- {np.round(np.std(acc_test),4)} [{np.round(np.min(acc_test),4)}-{np.round(np.max(acc_test),4)}]\")\n",
        "    print(f\"{np.round(np.mean(auc_test),4)} +/- {np.round(np.std(auc_test),4)} [{np.round(np.min(auc_test),4)}-{np.round(np.max(auc_test),4)}]\")\n",
        "    print(f\"{np.round(np.mean(mcc_test),4)} +/- {np.round(np.std(mcc_test),4)} [{np.round(np.min(mcc_test),4)}-{np.round(np.max(mcc_test),4)}]\")\n",
        "\n",
        "    plt.figure(figsize=(15,5))\n",
        "    data = np.hstack([np.array(acc_test)[:,None],np.array(auc_test)[:,None],np.array(mcc_test)[:,None]])\n",
        "    df   = pd.DataFrame(data,columns=[\"Test Accuracy\",\"Test AUC\",\"Test Matthew Correlation Coefficient\"])\n",
        "    sb.boxplot(data=df, orient=\"h\")\n",
        "    plt.title(\"Detecting Denial-of-Service Attacks\\nDistribution of Metrics on Test Set\")\n",
        "    plt.show()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE_GRUvAl-YB"
      },
      "source": [
        "# Start of Evaluation\n",
        "X,y = preprocess_data()\n",
        "\n",
        "auc_test = []\n",
        "acc_test = []\n",
        "mcc_test = []\n",
        "\n",
        "for trial in range(TRIALS):\n",
        "    index     = list(range(0, len(X), STEP))\n",
        "    indices   = list(StratifiedShuffleSplit(n_splits=1, test_size=.15).split(index, y[::STEP]))\n",
        "    train_idx = indices[0][0]\n",
        "    test_idx  = indices[0][1]\n",
        "    train_idx = np.array([[index[i], index[i]+1, index[i]+2] for i in train_idx]).ravel()\n",
        "    test_idx  = np.array([[index[i], index[i]+1, index[i]+2] for i in test_idx]).ravel()\n",
        "\n",
        "    x_train =  X[train_idx]\n",
        "    x_test  =  X[test_idx]\n",
        "    y_train =  y[train_idx].ravel()\n",
        "    y_test  =  y[test_idx].ravel()\n",
        "\n",
        "    for i in range(len(x_train)):\n",
        "        x_train[i] = np.divide(x_train[i], np.expand_dims(np.max(x_train[i], axis=-1)+1E-3, axis=-1))\n",
        "    for i in range(len(x_test)):\n",
        "        x_test[i] = np.divide(x_test[i], np.expand_dims(np.max(x_test[i], axis=-1)+1E-3, axis=-1))\n",
        "    x_train = x_train - 0.5\n",
        "    x_test  = x_test - 0.5\n",
        "\n",
        "    pca = PCA(n_components=PCA_DIM)\n",
        "    pca.fit(np.squeeze(np.mean(x_train, axis=-1)))\n",
        "    pca_x_train = pca.transform(np.squeeze(np.mean(x_train, axis=-1)))\n",
        "    pca_x_test = pca.transform(np.squeeze(np.mean(x_test, axis=-1)))\n",
        "\n",
        "    print(f\"Baseline 1:  {accuracy_score(y_test, np.zeros(len(y_test)))}\\n\")\n",
        "\n",
        "    if LEARN_METHOD == 1:\n",
        "      model = RandomForestClassifier(n_estimators=1000, max_depth=35)\n",
        "    elif LEARN_METHOD == 2:   \n",
        "      model = LogisticRegression(solver=\"lbfgs\") \n",
        "\n",
        "    model.fit(pca_x_train, y_train)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy_score(y_test, model.predict(pca_x_test))}\")\n",
        "    print(f\"Test AUC:      {roc_auc_score(y_test, model.predict_proba(pca_x_test)[:,1])}\")\n",
        "    print(f\"Test Log-Loss: {log_loss(y_test, model.predict_proba(pca_x_test)[:,1])}\")\n",
        "    print(f\"Test MCC:      {matthews_corrcoef(y_test, model.predict(pca_x_test))}\\n\")\n",
        "\n",
        "    acc_test.append(accuracy_score(y_test, model.predict(pca_x_test)))\n",
        "    auc_test.append(roc_auc_score(y_test, model.predict_proba(pca_x_test)[:,1]))\n",
        "    mcc_test.append(matthews_corrcoef(y_test, model.predict(pca_x_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g0f8dFnC1rL"
      },
      "source": [
        "The part of the code starts by segrating the testing and training the dataset. As specified above by 'STEP' variable, 3 examples are generated per each attack.  \n",
        "Dimension reduction is performed by using PCA after each scaling. While accuracy is reduced, the dataset becomes easier to analyize. \n",
        "Many forms of measurements are taken with this process, accuracy, AUC and Matthew Correlation Coefficient. "
      ]
    }
  ]
}